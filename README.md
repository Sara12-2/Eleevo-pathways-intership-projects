I have completed 5 projects.
# ğŸ“š Student Score Prediction

This project builds a regression model to predict students' exam scores based on factors such as study hours, sleep hours, and class participation. It follows the guidelines of Task 1 from the Arch Technologies internship and uses the [Student Performance Factors dataset](https://www.kaggle.com/datasets/lainguyn123/student-performance-factors) from Kaggle.

---

## ğŸš€ Project Objectives

- Perform data cleaning and visualization
- Train a linear regression model to predict exam scores
- Evaluate model performance using metrics like RÂ² and MSE
- Experiment with polynomial regression and feature selection

---

## ğŸ“¦ Dataset

**Source**: [Student Performance Factors â€“ Kaggle](https://www.kaggle.com/datasets/lainguyn123/student-performance-factors)  
**Features Used**:
- `Hours_Studied`
- `Sleep_Hours`
- `Participation` (if available)
- `Exam_Score` (target)

---

## ğŸ› ï¸ Tools & Libraries

- Python
- Pandas
- NumPy
- Matplotlib & Seaborn
- Scikit-learn

---

## ğŸ“ˆ Workflow

### 1. Data Cleaning
- Standardized column names
- Removed missing values
- Selected relevant features

### 2. Exploratory Data Analysis
- Pair plots and correlation heatmaps
- Insights into feature relationships

### 3. Model Training
- Linear Regression using Scikit-learn
- Polynomial Regression for performance comparison

### 4. Evaluation
- RÂ² Score
- Mean Squared Error
- Actual vs Predicted plots

---

## ğŸ” Results

| Model Type             | RÂ² Score | Notes                          |
|------------------------|----------|--------------------------------|
| Linear Regression      | ~0.75â€“0.85 | Strong correlation with study hours |
| Polynomial Regression  | Slightly higher | Captures non-linear patterns |
| Study Hours Only       | Lower     | Less accurate without other features |

---

## ğŸ¯ Bonus Experiments

- Polynomial regression with degree 2
- Feature selection: comparing models with and without `Participation`
- Visualization of predictions and residuals



# ğŸ›ï¸ Mall Customer Segmentation

## ğŸ¯ Objective
Segment mall customers into distinct groups based on their **Annual Income** and **Spending Score** using clustering techniques. This helps businesses understand customer behavior and tailor marketing strategies.

---

## ğŸ“Š Dataset
- **Source**: [Kaggle - Mall Customers Dataset](https://www.kaggle.com/datasets/kryusufkaya/mall-customers-dataset)
- **Features Used**:
  - `Annual Income (k$)`
  - `Spending Score (1-100)`

---

## ğŸ› ï¸ Tools & Libraries
- Python ğŸ
- Pandas
- Matplotlib & Seaborn
- Scikit-learn

---

## ğŸ“Œ Project Workflow

### 1. Data Loading & Exploration
- Loaded CSV using Pandas
- Explored basic statistics and feature distributions

### 2. Feature Selection
- Selected `Annual Income` and `Spending Score` for clustering

### 3. Data Scaling
- Applied `StandardScaler` to normalize features

### 4. Visual Exploration
- Used scatter plots to visualize customer distribution

### 5. Optimal Cluster Detection
- Applied **Elbow Method** to find the best number of clusters

### 6. K-Means Clustering
- Clustered customers into 5 segments
- Extracted cluster labels and centroids

### 7. Cluster Visualization
- Plotted clusters in 2D space with distinct colors
- Highlighted centroids using yellow markers

---

## ğŸŒŸ Bonus Tasks

### ğŸ” DBSCAN Clustering
- Applied **DBSCAN** for density-based clustering
- Detected noise points and non-spherical clusters
- Evaluated clustering using **Silhouette Score**

### ğŸ“Š Spending Score Analysis
- Calculated **average spending score per cluster**
- Visualized spending behavior using bar plots
- Provided insights into which clusters represent high-value customers


# ğŸ¦ Loan Approval Prediction (with SMOTE)

This project builds a machine learning pipeline to predict loan approval decisions based on applicant data. It addresses class imbalance using SMOTE and compares the performance of Logistic Regression and Decision Tree classifiers.

---

## ğŸ“Œ Problem Statement

Financial institutions need reliable systems to assess loan applications. This project aims to predict whether a loan should be **approved (`1`)** or **rejected (`0`)** using features such as income, credit history, and education.

---

## ğŸ“¦ Dataset

- **Source**: Simulated or anonymized loan application data  
- **Target Variable**: `Loan_Status` (0 = Rejected, 1 = Approved)  
- **Features Used**:
  - `ApplicantIncome`
  - `LoanAmount`
  - `Credit_History`
  - `Education`
  - `Gender`
  - `Married`
  - *(Additional features if available)*

---

## ğŸ› ï¸ Tools & Libraries

- Python ğŸ  
- Pandas, NumPy  
- Scikit-learn  
- imbalanced-learn (SMOTE)  
- Matplotlib, Seaborn  
- ipywidgets (for interactive GUI)

---

## ğŸ“ˆ Workflow

### 1. Data Preprocessing
- Handled missing values
- Encoded categorical variables
- Scaled numerical features
- Split into training and test sets

### 2. Modeling
- Trained two classifiers:
  - Logistic Regression
  - Decision Tree
- Applied **SMOTE** to balance training data

### 3. Evaluation
- Confusion Matrix
- Precision, Recall, F1-score
- Accuracy and class-wise metrics

### 4. Bonus: Interactive GUI
- Built a simple loan prediction interface using `ipywidgets`
- Users can input features and get real-time predictions

---

## ğŸ“Š Results

| Model                  | Accuracy | F1-Score (Approved) |
|-----------------------|----------|---------------------|
| Logistic Regression   | 91.3%    | 0.931               |
| Decision Tree         | 98.4%    | 0.987               |
| Logistic + SMOTE      | 92.3%    | 0.937               |
| Decision Tree + SMOTE | 98.0%    | 0.984               |

- **SMOTE** improved recall for minority class (rejected loans)
- **Decision Tree** showed highest accuracy but may overfit
- **Logistic Regression** offers interpretability and solid performance

---

## ğŸ¯ Bonus Experiments

- Compared models with and without SMOTE
- Visualized confusion matrices and classification reports
- Built an interactive predictor using `ipywidgets`
- Explored feature importance and decision boundaries

---

## ğŸ“ Files

- `loan_data.csv` â€“ Raw dataset  
- `loan_model.ipynb` â€“ Full notebook with pipeline  
- `loan_predictor_gui.ipynb` â€“ Interactive GUI using ipywidgets  
- `README.md` â€“ Project documentation

---

## ğŸš€ Future Enhancements

- Hyperparameter tuning with GridSearchCV  
- Add more features (e.g., employment type, dependents)  
- Deploy as a web app using Streamlit or Flask  
- Integrate explainability tools like SHAP or LIME

---
# ğŸ¬ Walmart Sales Forecasting  

This project aims to **forecast weekly Walmart sales** using regression techniques on time-series data. The goal is to capture sales trends and predict future values using lag features and date-based attributes.  

---

## ğŸ“Š Dataset  

- **Source**: [Walmart Recruiting - Store Sales Forecasting (Kaggle)](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting)  
- **File Used**: `Walmart.csv`  
- **Key Columns**:  
  - `Store` â†’ Store ID  
  - `Dept` â†’ Department ID  
  - `Date` â†’ Weekly sales date  
  - `Weekly_Sales` â†’ Target variable (weekly sales amount)  
  - `IsHoliday` â†’ Whether the week includes a holiday  

---

## ğŸ¯ Project Objectives  

- Perform **data preprocessing** and feature extraction  
- Create **time-series lag features** to capture past sales trends  
- Train a **Linear Regression model**  
- Evaluate using **RMSE** and **RÂ² score**  
- Visualize **actual vs predicted sales**  

---

## ğŸ› ï¸ Tools & Libraries  

- Python ğŸ  
- Pandas & NumPy  
- Matplotlib & Seaborn  
- Scikit-learn  

---

## ğŸ“ˆ Workflow  

### 1. Data Preprocessing  
- Convert `Date` column to datetime  
- Extract `Year`, `Month`, `Week`  
- Create lag features: `Lag_1`, `Lag_2` (previous sales)  
- Drop rows with missing values  

### 2. Feature Selection  
- **Features**: `Year`, `Month`, `Week`, `Lag_1`, `Lag_2`  
- **Target**: `Weekly_Sales`  

### 3. Train-Test Split  
- **Time-aware split**: first 80% training, last 20% testing  

### 4. Model Training  
- Linear Regression applied on features  

### 5. Evaluation  
- **RMSE**: Root Mean Squared Error  
- **RÂ²**: Coefficient of determination  

### 6. Visualization  
- Line plot comparing **actual vs predicted sales** over time  

---

## ğŸ“Š Results  

- **RMSE**: Represents the average prediction error  
- **RÂ² Score**: Closer to 1 indicates stronger predictive performance  
- Visualization clearly shows predicted trend following actual sales  

---

## ğŸš€ Future Improvements  

- Implement **ARIMA / SARIMA** for time-series forecasting  
- Try **Random Forest, XGBoost, or LSTM** for non-linear patterns  
- Add **holiday, store, and promotion features** for richer insights  

---

ğŸ‘‰ This project demonstrates how **time-aware regression models** can forecast retail sales trends and provide insights for **inventory planning & business strategy**.  



